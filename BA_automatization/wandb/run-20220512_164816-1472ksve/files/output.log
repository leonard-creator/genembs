[34m[1mwandb[39m[22m: logging graph, to disable use `wandb.watch(log_graph=False)`
[34m[1mwandb[39m[22m: [33mWARNING[39m Serializing object of type list that is 108024 bytes
[34m[1mwandb[39m[22m: [33mWARNING[39m Serializing object of type list that is 121432 bytes
Using 16bit native Automatic Mixed Precision (AMP)
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Installed CUDA version 11.4 does not match the version torch was compiled with 11.3 but since the APIs are compatible, accepting this combination
Using /home/tilingl/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/tilingl/.cache/torch_extensions/py39_cu113/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.9168550968170166 seconds
Validation sanity check:   0% 0/135 [00:00<?, ?it/s]
Set SLURM handle signals.
  | Name          | Type       | Params
---------------------------------------------
0 | head          | MLPHead    | 3.8 M
1 | valid_metrics | ModuleDict | 0
2 | train_metrics | ModuleDict | 0
---------------------------------------------
3.8 M     Trainable params
0         Non-trainable params
3.8 M     Total params






























































































































































































































































































































































































































































































































































































































































































































































































































































































Epoch 0:  85% 761/895 [31:35<05:33,  2.49s/it, loss=3.92, v_num=ksve, train/CoxPH=3.820, train/CoxPH_scaled=3.820, train/loss=3.820]




































































































































































































































































































































































































































































































































































































































































































































































































































































































Epoch 1:  84% 754/895 [33:07<06:11,  2.64s/it, loss=3.91, v_num=ksve, train/CoxPH=3.910, train/CoxPH_scaled=3.910, train/loss=3.910, valid/CoxPH=3.900, valid/CoxPH_scaled=3.900, valid/loss=3.900]
/home/tilingl/miniconda3/envs/ehrgraphs2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:685: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...

Epoch 1:  84% 755/895 [33:09<06:09,  2.64s/it, loss=3.9, v_num=ksve, train/CoxPH=3.740, train/CoxPH_scaled=3.740, train/loss=3.740, valid/CoxPH=3.900, valid/CoxPH_scaled=3.900, valid/loss=3.900]