{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00b62a94-23b6-46ea-898f-2153cd1e304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# script to train standard models, extract and save embeddings via Hyperparameter argparser input\n",
    "# should enable a better automatic workflow for starting multiple runs with only one script\n",
    "# using Pykeen==1.8.0 (slightly different functions to pykeen==1.7.0)\n",
    "# needs triple_list_graph_full_v3.feather for faster loading of the \"graph\"\n",
    "# --> /sc-projects/sc-proj-ukb-cvd/data/2_datasets_pre/220208_graphembeddings/ triples\n",
    "# corresponding python script: auto_generate_embs.py\n",
    "\n",
    "# TODO:\n",
    "\n",
    "import pykeen\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pykeen.triples import TriplesFactory\n",
    "from pykeen.pipeline import pipeline\n",
    "import torch\n",
    "import argparse\n",
    "from pykeen.regularizers import Regularizer, regularizer_resolver\n",
    "from pykeen.utils import resolve_device\n",
    "\n",
    "#import all models and parameter\n",
    "from pykeen.models  import ConvE, TransE, ComplEx, MuRE, RotatE, TuckER\n",
    "from pykeen.training import LCWATrainingLoop, SLCWATrainingLoop\n",
    "from pykeen.losses import BCEWithLogitsLoss\n",
    "\n",
    "# testing:\n",
    "from pykeen.datasets import Nations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac4d3b29-a20e-41ed-af45-e9cb56030671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to return the classes and not argparsed strings\n",
    "def help_model(m_str):\n",
    "    if m_str == \"ConvE\":\n",
    "        return ConvE\n",
    "    if m_str == \"TransE\":\n",
    "        return TransE\n",
    "    if m_str == \"ComplEx\":\n",
    "        return ComplEx\n",
    "    if m_str == \"MuRE\":\n",
    "        return MuRE\n",
    "    if m_str == \"RotatE\":\n",
    "        return RotatE\n",
    "    if m_str == \"TuckER\":\n",
    "        return TuckER\n",
    "    else:\n",
    "        return \"unknown, add Model to help_model func!\"\n",
    "\n",
    "def help_loss(l_str):\n",
    "    if l_str is None:\n",
    "        return None\n",
    "\n",
    "def help_loop(loop_str):\n",
    "    if loop_str == \"SLCWATrainingLoop\":\n",
    "        return SLCWATrainingLoop\n",
    "    else:\n",
    "        return LCWATrainingLoop\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9427dc67-e331-4a29-bf93-8157305cfedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to start the embedding training with all variables and parameters\n",
    "def start_emb_train(triple_path, inverse, Model,\n",
    "                    emb_dim, Training_loop,\n",
    "                    check_name: str, emb_dict_name: str,\n",
    "                    batch_s=None, sub_batch=None, \n",
    "                    slice_s=None, Loss=None,):\n",
    "    \n",
    "    # make sure, that gpu is avaiable and chosen \n",
    "    device = 'cpu' ## 'gpu'\n",
    "    _device: torch.device = resolve_device(device)\n",
    "    print(f\"Using device: {device}\", type(_device))\n",
    "    \n",
    "    # generate Triples Factory\n",
    "    ##tripleArray = pd.read_feather(triple_path).to_numpy()\n",
    "    ##print('length of the triple array: ', len(tripleArray), type(tripleArray))\n",
    "    ##tf = TriplesFactory.from_labeled_triples(tripleArray, create_inverse_triples=inverse)\n",
    "    dataset = Nations()\n",
    "    tf = dataset.training\n",
    "    \n",
    "    print('loading TriplesFactory done ... ', type(tf))\n",
    "    print(tf.num_entities, tf.num_relations)    # old=511291 338 oldest= 511291 326\n",
    "    \n",
    "    #pick a Model that was imported\n",
    "    #choose a loss Class that was imported, default =None\n",
    "    kwargs={'triples_factory': tf, 'loss': Loss, 'predict_with_sigmoid':False}\n",
    "    model = Model(**kwargs, embedding_dim=emb_dim, random_seed=420)\n",
    "    model= model.to(_device) # important otherwise fall back to cpu\n",
    "    \n",
    "    # Pick an optimizer from Torch\n",
    "    from torch.optim import Adam\n",
    "    optimizer = Adam(params=model.get_grad_params())\n",
    "    \n",
    "    # Pick a training approach that was imported !! contains the losses, choose between SLCWATrainingLoop and LCWATrainingLoop\n",
    "    training_loop = Training_loop(\n",
    "\n",
    "        model=model,\n",
    "        triples_factory=tf,\n",
    "        optimizer=optimizer,\n",
    "        automatic_memory_optimization=True, #default =True\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # just run for one epoch, evaluate losses and restart training where it was left\n",
    "    for i in range(1,100):\n",
    "        # Train like Cristiano Ronaldo\n",
    "        losses = training_loop.train(\n",
    "            triples_factory=tf,\n",
    "            num_epochs=i,\n",
    "            batch_size=batch_s, #256, # if None -> automatic search for the best and greatest\n",
    "            checkpoint_name= check_name, # for example TransE_t2.pt\n",
    "            checkpoint_frequency=0, \n",
    "            \n",
    "            sub_batch_size=sub_batch, # not for SLCWA and not supported bc of batch normalization!!\n",
    "            slice_size=slice_s, # not for SLCWA\n",
    "\n",
    "        )\n",
    "        if i>1 and (losses[-2] - losses[-1]) < 1e-7: # changed 1e-6 to 1e-7\n",
    "            \n",
    "            #TODO: function to obtain embeddings\n",
    "            #switch model back to cpu device:\n",
    "            _device = torch.device('cpu') # \n",
    "            model.to(_device)\n",
    "            #do not need anymore: entity_RepModel = model.entity_representations[0] # check for more representations\n",
    "            try:\n",
    "                print(model.entity_representations[1])\n",
    "            except IndexError:\n",
    "                print('\\n','Index Error, no more entity_reps ', '\\n')\n",
    "            # acces entity_values, mapp them to list and pass the list to the entity_repModel to recieve Embeddings. Next, create embedding_dict and transform to DataFrame\n",
    "            ##nodes = pd.read_feather('/sc-projects/sc-proj-ukb-cvd/data/2_datasets_pre/220208_graphembeddings/metadata/connected_nodes_list.feather')\n",
    "            print(tf.entity_to_id, type(tf.entity_to_id),'e_ids', tf.entity_ids, tf.entity_to_id.keys())\n",
    "            entity_id_dict = tf.entity_to_id\n",
    "            entity_ids_as_tensors = torch.as_tensor([int(v) for v in entity_id_dict.values()], dtype=torch.long, device=_device)\n",
    "            \n",
    "            # casting node Names from list into equivalent ids(indices) as a torch.LongTensor on CPU --> bc model is also cpu\n",
    "            ##entity_ids = torch.as_tensor(tf.entities_to_ids(nodes['nodes']), dtype=torch.long, device=_device) # .view()?\n",
    "            \n",
    "            #all embeddings as a numpy.ndarray, indices as torch.LongTensor\n",
    "            entity_embeddings = model.entity_representations[0](indices=entity_ids_as_tensors).detach().numpy() # detach tensor \n",
    "            \n",
    "            # do not need anymore : embeddings = entity_RepModel(entity_ids) \n",
    "            df_dict = pd.DataFrame(dict(nodes= entity_id_dict.keys(), embeddings=list(entity_embeddings)))\n",
    "            print(df_dict.head())\n",
    "            # save embedding dict\n",
    "            emb_path = '/home/tilingl/Pykeen/New_Embedding_Stuff/Embeddings/Embedding_dict_' + emb_dict_name + '.feather'\n",
    "            print('saved in: ',emb_path)\n",
    "            df_dict.to_feather(emb_path)\n",
    "          \n",
    "            #end the loop\n",
    "            break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1bd307e-73cf-4a2b-badc-383529ec9b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script for creating TEST embeddings\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Script for creating TEST embeddings\")\n",
    "\n",
    "from pykeen.training import LCWATrainingLoop, SLCWATrainingLoop\n",
    "from pykeen.losses import BCEWithLogitsLoss\n",
    "\n",
    "triple_path = '/sc-projects/sc-proj-ukb-cvd/data/2_datasets_pre/220208_graphembeddings/triples/triple_list_graph_full_v3.feather'\n",
    "    # sub_batching and slicing only for SLCWA\n",
    "    # triple_path, inverse, Model, emb_dim, Training_loop, check_name: str, embedding_dict Name: str, batch_size, sub_batch_size, slice_size,  Loss=None\n",
    "#start_emb_train(triple_path, True, TransE, 1024, SLCWATrainingLoop, 'TEST1.pt', 'TEST_full_v3', batch_s=None, sub_batch=None, slice_s=None, Loss=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b15cccb-3283-48d7-b09a-fb5d31d4c938",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (3711663861.py, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_4076015/3711663861.py\"\u001b[0;36m, line \u001b[0;32m22\u001b[0m\n\u001b[0;31m    start_emb_train(triple_path, args.inverse, args.Model, args.emb_dimension, args.SLCWA_LCWA, args.check_name,args.emb_dict_name, batch_s=None, sub_batch=None, slice_s=None, args.Loss)\u001b[0m\n\u001b[0m                                                                                                                                                                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "# argparser\n",
    "\n",
    "# argparser for all hyperparameter Options\n",
    "parser = argparse.ArgumentParser()\n",
    "#parser.add_argument('triple_path', help=\"declare the path to the triple.feather file\", type=str)\n",
    "parser.add_argument('inverse', help= \"add True/False wether inverted triple should created\", type=bool)\n",
    "parser.add_argument('Model', help=\"choose the model to train with. NOT NodePiece!\", type=str)\n",
    "parser.add_argument('emb_dimension', help=\"specify embedding dimension\" , type=int)\n",
    "parser.add_argument('SLCWA_LCWA', help=\"SLCWATrainingLoop or LCWA training Loop\", type=str)\n",
    "parser.add_argument('check_Name.pt', help=\" choose name for the checkpoint.pt\", type=str)\n",
    "parser.add_argument('emb_dict_name', help=\"name for the Embedding_dictionary, newest= xxx_full_v3\", type=str)\n",
    "parser.add_argument('Loss', help=\"specify Loss class , default None\", type=str)\n",
    "# add optional arguments\n",
    "parser.add_argument('-b', '--batch_size', help=\"select batch size, if none: automatic batchsize search\", type=int)\n",
    "parser.add_argument('-sb', '--sub_batch_size', help=\"only for LCWA, sub batch_size for effizient memory usage\", type=int)\n",
    "parser.add_argument('-s', '--slize_size', help=\"only for LCWA, divisor for slicing batches for single calculations\", type=int)\n",
    "\n",
    "args = parser.parse_args() # returns data from the options specified\n",
    "print(args.inverse)\n",
    "\n",
    "# transform strings in classes:\n",
    "Model = help_model(args.Model)\n",
    "loop = help_loop(args.SLCWA_LCWA)\n",
    "Loss = help_loss(args.Loss)\n",
    "\n",
    "#start run with passed arguments\n",
    "start_emb_train(triple_path, args.inverse, Model, args.emb_dimension, loop, args.check_name,args.emb_dict_name, batch_s=None, sub_batch=None, slice_s=None, Loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
