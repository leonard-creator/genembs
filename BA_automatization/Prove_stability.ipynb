{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f06ae4f9-713a-4aa9-a76a-d23a5aa90e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pykeen\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pykeen.triples import TriplesFactory\n",
    "from pykeen.pipeline import pipeline\n",
    "from pykeen.constants import PYKEEN_CHECKPOINTS\n",
    "import torch\n",
    "import argparse\n",
    "from pykeen.regularizers import Regularizer, regularizer_resolver\n",
    "from pykeen.utils import resolve_device\n",
    "\n",
    "#import all models and parameter\n",
    "from pykeen.models  import ConvE, TransE, ComplEx, MuRE, RotatE, TuckER, DistMult, RESCAL\n",
    "from pykeen.training import LCWATrainingLoop, SLCWATrainingLoop\n",
    "from pykeen.losses import BCEWithLogitsLoss, SoftplusLoss, NSSALoss, SoftMarginRankingLoss, PairwiseLogisticLoss\n",
    "\n",
    "# testing:\n",
    "from pykeen.datasets import Nations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4558251a-9b21-464e-8a75-58486cc08733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pykeen.losses.MarginRankingLoss'>\n"
     ]
    }
   ],
   "source": [
    "# find base losses \n",
    "M= DistMult\n",
    "print(M.loss_default)\n",
    "del M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98fb8be6-67b8-4d21-951b-c9a4f17cd4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No cuda devices were available. The model runs on CPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: gpu <class 'torch.device'>\n",
      "['OMOP_21600001' '0' 'OMOP_21600509']\n",
      "['OMOP_21600001' '0' 'OMOP_21600509']\n",
      "length of the triple array:  4000 <class 'numpy.ndarray'>\n",
      "loading TriplesFactory done ...  <class 'pykeen.triples.triples_factory.TriplesFactory'>\n",
      "1151 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sc-projects/sc-proj-ukb-cvd/environments/gnn/lib/python3.9/site-packages/pykeen/nn/representation.py:369: UserWarning: Directly use Embedding.max_id instead of num_embeddings.\n",
      "  warnings.warn(f\"Directly use {self.__class__.__name__}.max_id instead of num_embeddings.\")\n",
      "/sc-projects/sc-proj-ukb-cvd/environments/gnn/lib/python3.9/site-packages/pykeen/nn/representation.py:375: UserWarning: Directly use Embedding.shape instead of num_embeddings.\n",
      "  warnings.warn(f\"Directly use {self.__class__.__name__}.shape instead of num_embeddings.\")\n"
     ]
    }
   ],
   "source": [
    " # make sure, that gpu is avaiable and chosen \n",
    "\n",
    "triple_path = '/sc-projects/sc-proj-ukb-cvd/data/2_datasets_pre/220208_graphembeddings/triples/triple_list_graph_full_v3.feather'\n",
    "inverse=True\n",
    "emb_dim=3\n",
    "Loss=None\n",
    "Training_loop=SLCWATrainingLoop\n",
    "Model=TransE\n",
    "\n",
    "device = 'gpu' \n",
    "_device: torch.device = resolve_device(device)\n",
    "print(f\"Using device: {device}\", type(_device))\n",
    "   \n",
    "# generate Triples Factory\n",
    "tripleAr = pd.read_feather(triple_path).to_numpy()\n",
    "print(tripleAr[0])\n",
    "tripleArray = [tripleAr[i] for i in range(4000)]\n",
    "tripleArray = np.asarray(tripleArray)\n",
    "print(tripleArray[0])\n",
    "print('length of the triple array: ', len(tripleArray), type(tripleArray))\n",
    "tf = TriplesFactory.from_labeled_triples(tripleArray, create_inverse_triples=inverse)\n",
    "    \n",
    "print('loading TriplesFactory done ... ', type(tf))\n",
    "print(tf.num_entities, tf.num_relations)    #700380 404 old=511291 338 oldest= 511291 326\n",
    "    \n",
    "#pick a Model that was imported\n",
    "#choose a loss Class that was imported, default =None\n",
    "kwargs={'triples_factory': tf, 'loss': Loss, 'predict_with_sigmoid':False}\n",
    "model = Model(**kwargs, embedding_dim=emb_dim, random_seed=420)\n",
    "model= model.to(_device) # important otherwise fall back to cpu\n",
    "    \n",
    "# Pick an optimizer from Torch\n",
    "from torch.optim import Adam\n",
    "optimizer = Adam(params=model.get_grad_params())\n",
    "# Pick a training approach that was imported !! contains the losses, choose between SLCWATrainingLoop and LCWATrainingLoop\n",
    "training_loop = Training_loop(\n",
    "    model=model,\n",
    "    triples_factory=tf,\n",
    "    optimizer=optimizer,\n",
    "    automatic_memory_optimization=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6e88eb80-d56a-41eb-8bc5-a0fc6391819e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict_keys'>\n",
      "656283\n",
      "[['OMOP_1000560' '17' 'OMOP_1000599']\n",
      " ['OMOP_1000560' '17' 'OMOP_1000600']\n",
      " ['OMOP_1000560' '17' 'OMOP_1000612']\n",
      " ...\n",
      " ['phecode_997' '197' 'OMOP_764563']\n",
      " ['phecode_997' '197' 'OMOP_765111']\n",
      " ['phecode_997' '199' 'phecode_997']]\n"
     ]
    }
   ],
   "source": [
    "#check out the checkpoint:\n",
    "#checkpoint = torch.load(PYKEEN_CHECKPOINTS.joinpath('/sc-scratch/sc-scratch-ukb-cvd/checkpoints_pykeen_leonard/TransE_Test.pt'), map_location=torch.device('cpu'))\n",
    "#ent_dict=checkpoint['entity_to_id_dict']\n",
    "keys=ent_dict.keys()\n",
    "print(type(keys))\n",
    "#for i in range(10): print(keys[i])\n",
    "print(ent_dict['OMOP_765111'])\n",
    "print(triples)\n",
    "#print(tripleArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "602a7782-f4b4-4dcd-bfb2-9ac8aa8f80b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'OMOP_1000560'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3845259/2677166834.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# entity_id_mapping check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentities_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'OMOP_1000560'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'OMOP_1000577'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'OMOP_1000579'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'OMOP_1000599'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'OMOP_1000600'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#OMOP_1000577, OMOP_1000579, OMOP_1000599, OMOP_1000600, phecode_997\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#x=tf.tensor_t_df(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtriples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/sc-projects/sc-proj-ukb-cvd/environments/gnn/lib/python3.9/site-packages/pykeen/triples/triples_factory.py\u001b[0m in \u001b[0;36mentities_to_ids\u001b[0;34m(self, entities)\u001b[0m\n\u001b[1;32m   1112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mentities_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCollection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCollection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mCollection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: D102\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ensure_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_or_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_to_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentity_labeling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_to_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrelations_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelations\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCollection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCollection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mCollection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: D102\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/sc-projects/sc-proj-ukb-cvd/environments/gnn/lib/python3.9/site-packages/pykeen/triples/triples_factory.py\u001b[0m in \u001b[0;36m_ensure_ids\u001b[0;34m(labels_or_ids, label_to_id)\u001b[0m\n\u001b[1;32m    167\u001b[0m ) -> Collection[int]:\n\u001b[1;32m    168\u001b[0m     \u001b[0;34m\"\"\"Convert labels to IDs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabel_to_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml_or_i\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_or_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0ml_or_i\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml_or_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels_or_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/sc-projects/sc-proj-ukb-cvd/environments/gnn/lib/python3.9/site-packages/pykeen/triples/triples_factory.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    167\u001b[0m ) -> Collection[int]:\n\u001b[1;32m    168\u001b[0m     \u001b[0;34m\"\"\"Convert labels to IDs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabel_to_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml_or_i\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_or_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0ml_or_i\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml_or_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels_or_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'OMOP_1000560'"
     ]
    }
   ],
   "source": [
    "# entity_id_mapping check\n",
    "print(tf.entities_to_ids(['OMOP_1000560', 'OMOP_1000577', 'OMOP_1000579','OMOP_1000599','OMOP_1000600'])) #OMOP_1000577, OMOP_1000579, OMOP_1000599, OMOP_1000600, phecode_997\n",
    "#x=tf.tensor_t_df( \n",
    "triples=tf.triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dee68e1-59c9-45e5-b49e-9792412b1c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "becdd299a37841f097cf3f9fbd1cb283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epochs on cpu: 4epoch [00:00, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No random seed is specified. This may lead to non-reproducible results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sc-projects/sc-proj-ukb-cvd/environments/gnn/lib/python3.9/site-packages/pykeen/nn/representation.py:369: UserWarning: Directly use Embedding.max_id instead of num_embeddings.\n",
      "  warnings.warn(f\"Directly use {self.__class__.__name__}.max_id instead of num_embeddings.\")\n",
      "/sc-projects/sc-proj-ukb-cvd/environments/gnn/lib/python3.9/site-packages/pykeen/nn/representation.py:375: UserWarning: Directly use Embedding.shape instead of num_embeddings.\n",
      "  warnings.warn(f\"Directly use {self.__class__.__name__}.shape instead of num_embeddings.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e65c3b1fee0a4e2e87eee3dee0b42888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epochs on cpu: 4epoch [00:00, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Index Error, no more entity_reps  \n",
      "\n",
      "          nodes                              embeddings\n",
      "0  OMOP_1000632   [0.46381128, -0.7611315, -0.45338494]\n",
      "1  OMOP_1036059   [-0.17091985, 0.7019024, -0.69146186]\n",
      "2  OMOP_1036094  [-0.8853249, -0.025909247, 0.46425048]\n",
      "3  OMOP_1036228    [0.2761429, -0.48907068, -0.8273784]\n",
      "4  OMOP_1036525   [-0.13714032, -0.814403, -0.56386197]\n"
     ]
    }
   ],
   "source": [
    "# just run for one epoch, evaluate losses and restart training where it was left\n",
    "for i in range(1,20):\n",
    "    if i >1:\n",
    "        #make shure the loaded checkpoint is has the right mapping:\n",
    "        checkpoint = torch.load(PYKEEN_CHECKPOINTS.joinpath('/sc-scratch/sc-scratch-ukb-cvd/checkpoints_pykeen_leonard/TransE_Test55.pt'))\n",
    "        tf = TriplesFactory.from_labeled_triples(\n",
    "            triples =tripleArray,\n",
    "            create_inverse_triples=inverse,\n",
    "            entity_to_id=checkpoint['entity_to_id_dict'],\n",
    "            relation_to_id=checkpoint['relation_to_id_dict'],\n",
    "            )\n",
    "        kwargs={'triples_factory': tf, 'loss': Loss, 'predict_with_sigmoid':False}\n",
    "        model = Model(**kwargs, embedding_dim=emb_dim, random_seed=None)\n",
    "        model= model.to(_device) # important otherwise fall back to cpu\n",
    "    \n",
    "        # Pick an optimizer from Torch\n",
    "        from torch.optim import Adam\n",
    "        optimizer = Adam(params=model.get_grad_params())\n",
    "        # Pick a training approach that was imported !! contains the losses, choose between SLCWATrainingLoop and LCWATrainingLoop\n",
    "        training_loop = Training_loop(\n",
    "            model=model,\n",
    "            triples_factory=tf,\n",
    "            optimizer=optimizer,\n",
    "            automatic_memory_optimization=True, \n",
    "            )\n",
    "        print(\"check done!\") \n",
    "        # Train like Cristiano Ronaldo\n",
    "    losses = training_loop.train(\n",
    "        triples_factory=tf,\n",
    "        num_epochs=i,\n",
    "        batch_size=None, #256, # if None -> automatic search for the best and greatest\n",
    "        checkpoint_name= 'TransE_Test55.pt', # for example TransE_t2.pt\n",
    "        checkpoint_frequency=0,\n",
    "        checkpoint_directory='/sc-scratch/sc-scratch-ukb-cvd/checkpoints_pykeen_leonard', # new checkpoint dir bc of massive storage needs\n",
    "            \n",
    "        sub_batch_size=None, # not for SLCWA and not supported bc of batch normalization!!\n",
    "        slice_size=None, # not for SLCWA\n",
    "        )\n",
    "    \n",
    "    if i>1 and (losses[-2] - losses[-1]) < 1e-7: # changed 1e-6 to 1e-7  \n",
    "        #TODO: function to obtain embeddings\n",
    "        #switch model back to cpu device:\n",
    "        _device = torch.device('cpu') # \n",
    "        model.to(_device)\n",
    "        #do not need anymore: entity_RepModel = model.entity_representations[0] # check for more representations\n",
    "        try:\n",
    "            print(model.entity_representations[1])\n",
    "        except IndexError:\n",
    "            \n",
    "            print('\\n','Index Error, no more entity_reps ', '\\n')\n",
    "        # acces entity_values, mapp them to list and pass the list to the entity_repModel to recieve Embeddings. Next, create embedding_dict and transform to DataFrame\n",
    "        ##nodes = pd.read_feather('/sc-projects/sc-proj-ukb-cvd/data/2_datasets_pre/220208_graphembeddings/metadata/connected_nodes_list.feather')\n",
    "            \n",
    "        #print(tf.entity_to_id, type(tf.entity_to_id),'e_ids', tf.entity_ids, tf.entity_to_id.keys())\n",
    "        #\n",
    "        entity_id_dict = tf.entity_to_id\n",
    "        entity_ids_as_tensors = torch.as_tensor([int(v) for v in entity_id_dict.values()], dtype=torch.long, device=_device)\n",
    "            \n",
    "            # casting node Names from list into equivalent ids(indices) as a torch.LongTensor on CPU --> bc model is also cpu\n",
    "            ##entity_ids = torch.as_tensor(tf.entities_to_ids(nodes['nodes']), dtype=torch.long, device=_device) # .view()?\n",
    "            \n",
    "            #all embeddings as a numpy.ndarray, indices as torch.LongTensor\n",
    "        entity_embeddings = model.entity_representations[0](indices=entity_ids_as_tensors).detach().numpy() # detach tensor \n",
    "            \n",
    "        # do not need anymore : embeddings = entity_RepModel(entity_ids) \n",
    "        df_dict = pd.DataFrame(dict(nodes= entity_id_dict.keys(), embeddings=list(entity_embeddings)))\n",
    "        print(df_dict.head())\n",
    "        # save embedding dict\n",
    "        #emb_path = '/sc-projects/sc-proj-ukb-cvd/data/2_datasets_pre/220208_graphembeddings/embeddings_leonard/Embedding_dict_' + emb_dict_name + '.feather'\n",
    "        #print('saved in: ',emb_path)\n",
    "        #df_dict.to_feather(emb_path)\n",
    "        break\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724c7744-46e6-450f-9e92-a2a3ff0ed1ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
