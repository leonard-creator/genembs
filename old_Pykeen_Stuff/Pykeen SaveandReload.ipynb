{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c5db98-8926-4135-b8e3-c8955554a561",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a28ecd37-5bc6-4d5d-a0c4-1363d4029b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one epoche, safe and evaluate manuel . after that, start again\n",
    "\n",
    "import pykeen\n",
    "from pykeen.pipeline import pipeline\n",
    "import networkx as nx\n",
    "import pathlib\n",
    "from random import sample\n",
    "import pandas as pd\n",
    "from pykeen.triples import TriplesFactory\n",
    "import torch\n",
    "import numpy as np\n",
    "from pykeen.models import TransE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbe12b6-0919-48e9-a44d-7f6622a129cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is possible to continue training automaticly by simply reload the code and the pipeline will catch the latest saved CHECPOINT and will resume.\n",
    "# that includes increasing the num_epochs variable manually -> the latest checkpoint from the code will be load automatically\n",
    "# instead, loading models manually from checkpoints (for investigation) \n",
    "\n",
    "# avoid loading triples by array, think about way to save them in a path variable to avoid long preprocess and graph loading time\n",
    "# check dimension\n",
    "# check split ratio \n",
    "\n",
    "# NOTE: checpoints when bringing your own data: \"When continuing the training or in general using the model after resuming training, it is critical that the entity label to identifier (entity_to_id) and relation label to identifier (relation_to_id) mappings are the same as the ones that were used when saving the checkpoint. If they are not, then any downstream usage will be nonsense.\"\n",
    "# checkpoints also possible if trqaining is not in pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1339e8b7-47a3-4c7d-86c4-9ca8ad806c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Data \n",
    "from pykeen.datasets import CoDExMedium\n",
    "dataset = CoDExMedium\n",
    "'''evaluator = 'rankbased',\n",
    "    stopper = 'early',\n",
    "    stopper_kwargs=dict(frequency=5, patience=2, relative_delta=0.002)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "193a824f-fb1d-4a0c-8729-3827737f9f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 load the graph data and preprocess\n",
    "# from https://github.com/nebw/ehrgraphs/blob/master/ehrgraphs/data/data.py#L82-L117\n",
    "def preprocess_graph_heterogeneous(graph: nx.Graph):\n",
    "    edge_types = []\n",
    "    for u, v, data in graph.edges.data():\n",
    "        edge_types.append(data[\"edge_type\"])\n",
    "\n",
    "    edge_codes, edge_types = pd.factorize(edge_types)\n",
    "\n",
    "    node_types = []\n",
    "    for n, data in graph.nodes.data():\n",
    "        node_types.append(data[\"node_type\"])\n",
    "\n",
    "    node_codes, node_types = pd.factorize(node_types)\n",
    "\n",
    "    preprocessed_graph = nx.DiGraph()\n",
    "    preprocessed_graph.add_nodes_from(graph.nodes())\n",
    "\n",
    "    preprocessed_graph.node_codes = node_codes\n",
    "    preprocessed_graph.node_types = node_types\n",
    "        \n",
    "    # drop shortcut edges\n",
    "    exclude_codes = []\n",
    "    exclude_codes.append(edge_codes[list(edge_types).index(\"Subsumes\")])\n",
    "    exclude_codes.append(edge_codes[list(edge_types).index(\"Is a\")])\n",
    "\n",
    "    for (u, v, w), c in zip(graph.edges.data(\"edge_weight\"), edge_codes):\n",
    "        assert w is not None\n",
    "\n",
    "        # drop shortcut edges\n",
    "        if c in exclude_codes and w < 1.0:\n",
    "            continue\n",
    "\n",
    "        preprocessed_graph.add_edge(u, v, edge_weight=w, edge_code=c)\n",
    "\n",
    "    preprocessed_graph.edge_types = edge_types\n",
    "    \n",
    "    \n",
    "\n",
    "    return preprocessed_graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be2d9a58-5bb2-4060-9960-34b1c9fdb310",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 load graph data \n",
    "#loading the full graph\n",
    "base_path = pathlib.Path(\n",
    "    \"/data/analysis/ag-reils/ag-reils-shared/cardioRS/data/2_datasets_pre/211110_anewbeginning\")\n",
    "G = nx.readwrite.gpickle.read_gpickle('/data/analysis/ag-reils/ag-reils-shared/cardioRS/data/2_datasets_pre/211110_anewbeginning/graph_full_211122.p')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee179646-f63a-499c-b59e-26a78335a1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 preprocess now \n",
    "PG = preprocess_graph_heterogeneous(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b200134-4e50-4bb8-bbe4-ddd9e60e8288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28842781\n"
     ]
    }
   ],
   "source": [
    "#2 create Triple Array\n",
    "tripleList=[]\n",
    "nodes=[]\n",
    "for u,v,data in PG.edges.data():\n",
    "    l=[]\n",
    "    l.append(u)\n",
    "    nodes.append(u)\n",
    "    l.append(data['edge_code'])\n",
    "    l.append(v)\n",
    "    nodes.append(v)\n",
    "    tripleList.append(l)\n",
    "\n",
    "#needs triples as ndarray - shape (n,3), dtype:str \n",
    "tripleArray=np.array(tripleList, dtype=str)\n",
    "print(len(tripleArray))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd1c29a-0218-4f9f-afe9-c7e9ffcc50c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# Easy Pipeline Way #\n",
    "####################\n",
    "\n",
    "\n",
    "# --\n",
    "\n",
    "# Pick a model\n",
    "from pykeen.models import TransE\n",
    "\n",
    "for i in range(1,5):\n",
    "    results = pipeline(\n",
    "        dataset = dataset,\n",
    "        loss='marginranking',\n",
    "        loss_kwargs=dict(margin=1),\n",
    "        model = TransE,\n",
    "        model_kwargs=dict(embedding_dim=50),\n",
    "        training_kwargs=dict(\n",
    "            num_epochs=i,\n",
    "            checkpoint_name='my_3nd_attempt',\n",
    "            checkpoint_frequency=0, ),\n",
    "        training_loop = 'sLCWA',\n",
    "        negative_sampler = 'basic'\n",
    "    )\n",
    "    losses = results.losses\n",
    "    print(losses)\n",
    "    if i > 1 and (losses[-2] - losses[-1]) < 0.001:\n",
    "        break\n",
    "\n",
    "#results.plot_losses()\n",
    "#results.save_to_directory('single Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a0285021-852e-4d53-8b93-f40269d85ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> [0.0033517409174162126, 0.0022412704176836586, 0.0016890166910126432]\n",
      "[0.0033517409174162126, 0.0022412704176836586, 0.0016890166910126432]\n",
      "MarginRankingLoss(\n",
      "  (margin_activation): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# try to reach the losses\n",
    "\n",
    "losses = results.losses\n",
    "print(type(losses), losses)\n",
    "\n",
    "print(results.training_loop.losses_per_epochs)\n",
    "print(results.training_loop.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3af13dff-8c1f-4120-b8c1-5f938651f017",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pykeen.utils:No cuda devices were available. The model runs on CPU\n",
      "WARNING:pykeen.models.base:No random seed is specified. This may lead to non-reproducible results.\n",
      "INFO:pykeen.training.training_loop:=> no checkpoint found at '/home/tilingl/.data/pykeen/checkpoints/own_pipeline2.pt'. Creating a new file.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56c24ee144db49859f80de01fc37fe67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epochs on cpu:   0%|          | 0/1 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cpu:   0%|          | 0/725 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pykeen.training.training_loop:=> Saved checkpoint after having finished epoch 1.\n",
      "INFO:pykeen.training.training_loop:=> loading checkpoint '/home/tilingl/.data/pykeen/checkpoints/own_pipeline2.pt'\n",
      "INFO:pykeen.training.training_loop:=> loaded checkpoint '/home/tilingl/.data/pykeen/checkpoints/own_pipeline2.pt' stopped after having finished epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd055f00f0de4b59a48b4a661fdcc695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epochs on cpu:  50%|#####     | 1/2 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cpu:   0%|          | 0/725 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pykeen.training.training_loop:=> Saved checkpoint after having finished epoch 2.\n"
     ]
    }
   ],
   "source": [
    "# lets try out a defragmented pipeline with their dataset\n",
    "dataset2 = CoDExMedium()\n",
    "training_triples_factory = dataset2.training\n",
    "\n",
    "# Pick a model\n",
    "model = TransE(triples_factory=training_triples_factory, embedding_dim=64)\n",
    "\n",
    "# Pick an optimizer from Torch\n",
    "from torch.optim import Adam\n",
    "optimizer = Adam(params=model.get_grad_params())\n",
    "\n",
    "# Pick a training approach !! contains the losses\n",
    "from pykeen.training import SLCWATrainingLoop \n",
    "training_loop = SLCWATrainingLoop(\n",
    "\n",
    "    model=model,\n",
    "\n",
    "    triples_factory=training_triples_factory,\n",
    "\n",
    "    optimizer=optimizer,\n",
    "\n",
    ")\n",
    "\n",
    "# just run for one epoch, evaluate losses and restart training where it was left\n",
    "for i in range(1,5):\n",
    "    # Train like Cristiano Ronaldo\n",
    "    losses = training_loop.train(\n",
    "\n",
    "        triples_factory=training_triples_factory,\n",
    "        num_epochs=i,\n",
    "        batch_size=256,\n",
    "        checkpoint_name= 'own_pipeline2.pt',\n",
    "        checkpoint_frequency=0\n",
    "\n",
    "    )\n",
    "    if i>1 and (losses[-2] - losses[-1]) < 0.002:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30531a99-3f12-40f8-8617-fa9be9598141",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using automatically assigned random_state=1836517715\n",
      "/home/tilingl/miniconda3/envs/DeepWalk/lib/python3.8/site-packages/pykeen/triples/splitting.py:360: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  actual_ratio = actual_size / exp_size * exp_ratio\n",
      "No cuda devices were available. The model runs on CPU\n",
      "No random seed is specified. This may lead to non-reproducible results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "511291 326\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"for i in range(1,2):\\n    # Train like Cristiano Ronaldo\\n    losses2 = training_loop2.train(\\n\\n        triples_factory=training_factory,\\n        num_epochs=i,\\n        batch_size=256,\\n        checkpoint_name= 'fooling.pt',\\n        checkpoint_frequency=0\\n\\n    )\\n    if i>1 and (losses2[-2] - losses2[-1]) < 0.002:\\n        break\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2 Try out the defragmented version - first create triple factory\n",
    "'''When continuing the training or in general using the model after resuming training, it is critical that the entity label to\n",
    "identifier (entity_to_id) and relation label to identifier (relation_to_id) mappings are the same as the ones that were used \n",
    "when saving the checkpoint. If they are not, then any downstream usage will be nonsense. '''\n",
    "tf = TriplesFactory.from_labeled_triples(tripleArray, create_inverse_triples=True)\n",
    "\n",
    "training_factory, testing_factory = tf.split([1.0, 0.0])\n",
    "#entity_mapping = tf.entity_to_id\n",
    "#relation_mapping = tf.relation_to_id\n",
    "print(training_factory.num_entities, training_factory.num_relations)\n",
    "\n",
    "#training = TriplesFactory.from_labeled_triples(training_factory, create_inverse_triples= True, )\n",
    "#\n",
    "#testing = TriplesFactory.from_labeled_triples(testing_factory, create_inverse_triples=True,\n",
    "#                                              entity_to_id=train.entity_to_id,\n",
    " #                                             relation_to_id=train.relation_to_id,)\n",
    "                                              \n",
    "\n",
    "# Pick a model\n",
    "model2 = TransE(triples_factory=training_factory, embedding_dim=2)\n",
    "\n",
    "# Pick an optimizer from Torch\n",
    "from torch.optim import Adam\n",
    "optimizer2 = Adam(params=model2.get_grad_params())\n",
    "\n",
    "# Pick a training approach !! contains the losses\n",
    "from pykeen.training import SLCWATrainingLoop \n",
    "training_loop2 = SLCWATrainingLoop(\n",
    "\n",
    "    model=model2,\n",
    "\n",
    "    triples_factory=training_factory,\n",
    "\n",
    "    optimizer=optimizer2,\n",
    "\n",
    ")\n",
    "\n",
    "# just run for one epoch, evaluate losses and restart training where it was left\n",
    "'''for i in range(1,2):\n",
    "    # Train like Cristiano Ronaldo\n",
    "    losses2 = training_loop2.train(\n",
    "\n",
    "        triples_factory=training_factory,\n",
    "        num_epochs=i,\n",
    "        batch_size=256,\n",
    "        checkpoint_name= 'fooling.pt',\n",
    "        checkpoint_frequency=0\n",
    "\n",
    "    )\n",
    "    if i>1 and (losses2[-2] - losses2[-1]) < 0.002:\n",
    "        break\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4419df7-98c1-4a9a-b63c-293f9f8304e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(losses2)\n",
    "print(losses2)\n",
    "print(model2.num_entities, model2.entity_representations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e2861f1f-7397-4558-af09-40f4586b7b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No cuda devices were available. The model runs on CPU\n",
      "No random seed is specified. This may lead to non-reproducible results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(\n",
      "  (_embeddings): Embedding(17050, 50)\n",
      ")\n",
      "17050\n",
      "[-0.13400671 -0.12385295  0.19806443  0.09177307  0.15803958 -0.16651738\n",
      " -0.06492405  0.21340628  0.18431835  0.19797724  0.21336973  0.1774984\n",
      "  0.09119363 -0.04669401  0.13973255 -0.1246407   0.19583163  0.07195732\n",
      "  0.13446353  0.2295184   0.04414574  0.0688907  -0.07721438  0.12669078\n",
      "  0.08116424  0.0330678  -0.05262972  0.12321129  0.138985    0.06542039\n",
      "  0.07272934 -0.18556963 -0.1421097   0.05704216  0.04917973 -0.04737845\n",
      "  0.2461231   0.11587673  0.18489233 -0.08991764 -0.09038517 -0.07949767\n",
      " -0.17118451  0.19986464  0.05352865 -0.1716157   0.06938639 -0.17834206\n",
      "  0.22943144 -0.19793965]\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# Acsessing the model from the saved checkpoint#\n",
    "# 3 # 3 # 3 # 3 # 3 # 3 # 3 # 3 # 3 # 3 # 3 # 3#\n",
    "from pykeen.constants import PYKEEN_CHECKPOINTS\n",
    "\n",
    "checkpoint = torch.load(PYKEEN_CHECKPOINTS.joinpath('/home/tilingl/.data/pykeen/checkpoints/own_pipeline2.pt')) # CUDA problems\n",
    "print(type(checkpoint)) # checkpoint['entity_to_id_dict']) \n",
    "\n",
    "# checkpoint contains model and entity_to_id, realtion_to_id mapping that was used\n",
    "# load these into Pykeen:\n",
    "'''train_tf = TriplesFactory.from_labeled_triples(\n",
    "    triples = tripleArray, \n",
    "    entity_to_id=checkpoint['entity_to_id_dict'],\n",
    "    relation_to_id=checkpoint['relation_to_id_dict'],\n",
    ")'''\n",
    "\n",
    "\n",
    "train_tf = TriplesFactory.from_path(\n",
    "    path= '/home/tilingl/.data/pykeen/datasets/codexmedium/train.txt',\n",
    "    entity_to_id=checkpoint['entity_to_id_dict'],\n",
    "    relation_to_id=checkpoint['relation_to_id_dict'],\n",
    ")\n",
    "    \n",
    "# load the model and pass the train triple factory to the model\n",
    "used_model = TransE(triples_factory=train_tf)\n",
    "#used_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# get the embeddings\n",
    "# class RepresentationModule\n",
    "entity_RepModel = used_model.entity_representations[0] # check for more representations\n",
    "print(entity_RepModel)\n",
    "\n",
    "print(train_tf.num_entities)\n",
    "# accses over checkpoint: \n",
    "checkp_entities_val = list(checkpoint['entity_to_id_dict'].values())\n",
    "# Q100', 'Q1000', 'Q100005', 'Q1000051'\n",
    "\n",
    "embeddings = entity_RepModel(torch.tensor(checkp_entities_val, dtype=torch.int)) # cast list elements into tensors\n",
    "embedding_dict = dict(zip(checkpoint['entity_to_id_dict'].keys(), embeddings.detach().numpy() )) #  Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.\n",
    "print(embedding_dict['Q100005'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bd892e9-5310-448c-bbc9-6959f8e2a140",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TransE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_204954/3120751377.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mTransE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'TransE' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dbcf7be1-0492-4c54-b26e-c9dbfde24dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No cuda devices were available. The model runs on CPU\n",
      "No random seed is specified. This may lead to non-reproducible results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(\n",
      "  (_embeddings): Embedding(511291, 256)\n",
      ")\n",
      "511291\n",
      "OMOP_1000560\n",
      "<class 'torch.Tensor'>\n",
      "[-0.01253013 -0.09684575  0.08089289  0.04960537  0.03288252  0.04709427\n",
      "  0.01253581  0.02063313  0.00482531 -0.03269486  0.08955711 -0.09162737\n",
      " -0.01532889 -0.0669371  -0.04718629 -0.04141479  0.02702554 -0.06954572\n",
      " -0.03913069  0.09565155 -0.09373201 -0.10842036 -0.00117118  0.01456364\n",
      " -0.0466005   0.05035962  0.05272698 -0.05698881  0.01946181  0.06248292\n",
      " -0.0153423   0.02050512  0.03652842 -0.06304903  0.1050147   0.02716055\n",
      "  0.0241642  -0.00425509 -0.05885952 -0.06697097  0.02672952  0.07291657\n",
      " -0.05068693  0.0986026   0.07295278 -0.03017739 -0.05532933 -0.01534762\n",
      " -0.05434519 -0.06979137  0.08505629  0.05689349  0.07588166  0.08507898\n",
      " -0.03491642  0.02018699  0.09482616 -0.09885208 -0.01385707  0.01877359\n",
      " -0.01501659  0.04815469  0.04180819 -0.10517272  0.07282256  0.03604697\n",
      " -0.09307583 -0.00435607  0.02074452  0.05039392  0.03389975 -0.10712785\n",
      "  0.03156006  0.0241015  -0.04619716 -0.06638029 -0.05643636 -0.00093541\n",
      "  0.09567159 -0.05960814  0.06997407  0.07973195  0.02110654  0.0296947\n",
      " -0.08779005  0.10581161  0.02043875 -0.03860668 -0.01983727  0.07731341\n",
      " -0.06629597  0.05133804 -0.03071832  0.10767373 -0.05991624 -0.03900641\n",
      "  0.08908928 -0.06341275  0.03958593  0.01979576 -0.02708559  0.08352129\n",
      "  0.02075768  0.10773852 -0.08038048  0.09794497  0.07909603  0.03664766\n",
      " -0.01717238  0.10110333 -0.01450833 -0.03216534 -0.08739585 -0.02797749\n",
      " -0.05129457  0.0012284   0.03957826 -0.1042509   0.09479988  0.09147262\n",
      "  0.05280874  0.03363606 -0.04182439  0.01518899 -0.09044582 -0.0445204\n",
      " -0.09790057 -0.10326437  0.09844959  0.08851104  0.02777583 -0.02594395\n",
      "  0.01246659  0.10547841  0.10613218 -0.02290041  0.02091753  0.00737779\n",
      "  0.04420832 -0.04499267  0.02741105 -0.04698865  0.06709623 -0.01995499\n",
      "  0.10628782  0.01632849 -0.00548708  0.09353067  0.03948072  0.08836728\n",
      " -0.08441993 -0.10248902 -0.00638413  0.10288619  0.00465248  0.06919496\n",
      " -0.01916159  0.10407091  0.02757939  0.07636181 -0.10412192 -0.10401755\n",
      "  0.03430897  0.04153771 -0.05467524 -0.05270949 -0.09910471 -0.07991444\n",
      " -0.01263921 -0.0961851  -0.10370238  0.051107    0.01107684 -0.00054358\n",
      " -0.01241889  0.05981628 -0.10248838 -0.05248922  0.10696546 -0.04009637\n",
      " -0.06603047 -0.00804972 -0.09786978  0.08277943  0.08477436 -0.06044165\n",
      " -0.08963014  0.08018081 -0.04238795  0.01591982 -0.0765357  -0.05049229\n",
      " -0.06676546 -0.00048     0.09518223  0.06755157 -0.10599022 -0.09561364\n",
      " -0.02152602  0.04882408  0.09755737  0.10486903  0.05203734  0.05442318\n",
      "  0.04537088 -0.06127118  0.05975341 -0.10209511 -0.02748023  0.04896206\n",
      "  0.09080423 -0.03832452  0.00341793  0.06479286  0.09772545 -0.05389886\n",
      " -0.06597118  0.06468319  0.01618094  0.0176671  -0.02947394  0.04679739\n",
      "  0.06787243  0.00102948 -0.03987018  0.02840243  0.02220481  0.08092448\n",
      " -0.03488488  0.0245145  -0.0355046  -0.02893633 -0.10608505 -0.0357865\n",
      " -0.03207999 -0.00798197  0.06892839  0.05713747  0.00097611  0.04194142\n",
      "  0.09003201 -0.04854065 -0.07720763 -0.0582249  -0.04324189  0.05591155\n",
      " -0.02377075 -0.05206423  0.01858862 -0.03790038 -0.09280138 -0.01149112\n",
      "  0.00118469 -0.01060029  0.0830837  -0.07134608]\n",
      "549872  ==  511291\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# Creating Big - Model from checkpoint         #\n",
    "# 3 # 3 # 3 # 3 # 3 # 3 # 3 # 3 # 3 # 3 # 3 # 3#\n",
    "from pykeen.constants import PYKEEN_CHECKPOINTS\n",
    "\n",
    "checkpoint2 = torch.load(PYKEEN_CHECKPOINTS.joinpath('/home/tilingl/.data/pykeen/checkpoints/gpu_run_own_big.pt'), map_location={'cuda:0':'cpu'}) \n",
    "print(type(checkpoint2)) # checkpoint['entity_to_id_dict']) \n",
    "\n",
    "# checkpoint contains model and entity_to_id, realtion_to_id mapping that was used\n",
    "# load these into Pykeen:\n",
    "train_tf2 = TriplesFactory.from_labeled_triples(\n",
    "    triples = tripleArray, \n",
    "    entity_to_id=checkpoint2['entity_to_id_dict'],\n",
    "    relation_to_id=checkpoint2['relation_to_id_dict'],\n",
    ")\n",
    "\n",
    "    \n",
    "# load the model and pass the train triple factory to the model\n",
    "used_model2 = TransE(\n",
    "    triples_factory=train_tf2,\n",
    "    embedding_dim=256)\n",
    "#used_model2.load_state_dict(checkpoint2['model_state_dict'])\n",
    "\n",
    "# get the embeddings\n",
    "# class RepresentationModule\n",
    "entity_RepModel2 = used_model2.entity_representations[0] # check for more representations\n",
    "print(entity_RepModel2)\n",
    "\n",
    "print(train_tf2.num_entities)\n",
    "# accses over checkpoint: \n",
    "checkp_entities_val2 = list(checkpoint2['entity_to_id_dict'].values())\n",
    "# Q100', 'Q1000', 'Q100005', 'Q1000051'\n",
    "print(list(checkpoint2['entity_to_id_dict'].keys())[0])\n",
    "embeddings2 = entity_RepModel2(torch.tensor(checkp_entities_val2, dtype=torch.int)) # cast list elements into tensors\n",
    "print(type(embeddings2))\n",
    "embedding_dict2 = dict(zip(checkpoint2['entity_to_id_dict'].keys(), embeddings2.detach().numpy() )) #  Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.\n",
    "print(embedding_dict2['OMOP_1000560'])\n",
    "\n",
    "\n",
    "#################################################\n",
    "# check graph nodes against Triples Array #\n",
    "#filter duplicates \n",
    "amount = len(list(dict.fromkeys(checkpoint2['entity_to_id_dict'].keys())))\n",
    "print(len(PG.nodes()), ' == ', amount)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "33be2b8c-4f45-484e-b5dc-ccd809dbfa5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38581 + 511291 = 549872 == 549872\n",
      "0\n",
      "<class 'numpy.ndarray'>\n",
      "38581\n"
     ]
    }
   ],
   "source": [
    "# validating the nodes are existent in the embedding dict\n",
    "missing=[]\n",
    "for node in PG.nodes():\n",
    "    try:\n",
    "        x = embedding_dict2[str(node)]\n",
    "    except KeyError: \n",
    "        'Node is not in embedding dict'\n",
    "        missing.append(node)\n",
    "    else:\n",
    "        'if no error occures'\n",
    "        continue\n",
    "    finally:\n",
    "        'allways executed'\n",
    "        continue\n",
    "\n",
    "print(len(missing) , '+', train_tf2.num_entities ,'=' ,511291+len(missing), '==', len(list(PG.nodes())) )\n",
    "\n",
    "# check with networkx the missing nodes \n",
    "\n",
    "print(len(PG.edges(missing)))  # only edges incident to these nodes\n",
    "\n",
    "## add 0.00 to every missing nodes dimension\n",
    "#for n in missing:\n",
    "#    embedding_dict2[n] = \n",
    "\n",
    "\n",
    "#print(embedding_dict2['OMOP_1000560'])\n",
    "t = np.zeros((256,), dtype=float)\n",
    "print(type(t))\n",
    "print(len(list(nx.isolates(PG))))\n",
    "\n",
    "# export to csv \n",
    "df = pd.DataFrame(missing)\n",
    "###df.to_csv('isolated_nodes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f4f824-cf20-42c6-b7a3-405d5a8ecf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tripleArray))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47ad06d-ad63-4411-97f9-d0395709b42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out of : https://pykeen.readthedocs.io/en/stable/_modules/pykeen/training/training_loop.html#TrainingLoop\n",
    "\n",
    "# the training_loop class has following functinos and source code attributes\n",
    "result = self._train(\n",
    "                num_epochs=num_epochs,\n",
    "                batch_size=batch_size,\n",
    "                slice_size=slice_size,\n",
    "                label_smoothing=label_smoothing,\n",
    "                sampler=sampler,\n",
    "                continue_training=continue_training,\n",
    "                only_size_probing=only_size_probing,\n",
    "                use_tqdm=use_tqdm,\n",
    "                use_tqdm_batch=use_tqdm_batch,\n",
    "                tqdm_kwargs=tqdm_kwargs,\n",
    "                stopper=stopper,\n",
    "                result_tracker=result_tracker,\n",
    "                sub_batch_size=sub_batch_size,\n",
    "                num_workers=num_workers,\n",
    "                save_checkpoints=save_checkpoints,\n",
    "                checkpoint_path=checkpoint_path,\n",
    "                checkpoint_frequency=checkpoint_frequency,\n",
    "                checkpoint_on_failure_file_path=checkpoint_on_failure_file_path,\n",
    "                best_epoch_model_file_path=best_epoch_model_file_path,\n",
    "                last_best_epoch=last_best_epoch,\n",
    "                drop_last=drop_last,\n",
    "                callbacks=callbacks,\n",
    "                gradient_clipping_max_norm=gradient_clipping_max_norm,\n",
    "                gradient_clipping_norm_type=gradient_clipping_norm_type,\n",
    "                gradient_clipping_max_abs_value=gradient_clipping_max_abs_value,\n",
    "                triples_factory=triples_factory,\n",
    "                training_instances=training_instances,\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
