{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "934e2f07-f1c4-4bf4-bf95-8e2d648b85c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-2' coro=<Kernel.poll_control_queue() running at /sc-projects/sc-proj-ukb-cvd/environments/gnn/lib/python3.9/site-packages/ipykernel/kernelbase.py:227> wait_for=<Future finished result=[<zmq.sugar.fr...x7f3ed99800f0>, <zmq.sugar.fr...x7f3ec4736d50>, <zmq.sugar.fr...x7f3ec4736e00>, <zmq.sugar.fr...x7f3ec4736eb0>, <zmq.sugar.fr...x7f3ec4736f60>, <zmq.sugar.fr...x7f3ec4739040>, ...]> cb=[_chain_future.<locals>._call_set_state() at /sc-projects/sc-proj-ukb-cvd/environments/gnn/lib/python3.9/asyncio/futures.py:391]>\n"
     ]
    }
   ],
   "source": [
    "# script to load the diffrent node Embeddings and save them to feather file \n",
    "# needs triple_array.txt and nodes.txt file with all nodes from the Graph! (len = 549872)\n",
    "# at first, create list of nodes (name them according to graph)\n",
    "\n",
    "# TODO: add load triple from path\n",
    "import pykeen\n",
    "from pykeen.pipeline import pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pykeen.constants import PYKEEN_CHECKPOINTS\n",
    "from pykeen.triples import TriplesFactory\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3d9c15-3c93-4d71-b430-07ac57b1fb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 load the graph data and preprocess\n",
    "# from https://github.com/nebw/ehrgraphs/blob/master/ehrgraphs/data/data.py#L82-L117\n",
    "import networkx as nx\n",
    "import pathlib\n",
    "def preprocess_graph_heterogeneous(graph: nx.Graph):\n",
    "    edge_types = []\n",
    "    for u, v, data in graph.edges.data():\n",
    "        edge_types.append(data[\"edge_type\"])\n",
    "\n",
    "    edge_codes, edge_types = pd.factorize(edge_types)\n",
    "\n",
    "    node_types = []\n",
    "    for n, data in graph.nodes.data():\n",
    "        node_types.append(data[\"node_type\"])\n",
    "\n",
    "    node_codes, node_types = pd.factorize(node_types)\n",
    "\n",
    "    preprocessed_graph = nx.DiGraph()\n",
    "    preprocessed_graph.add_nodes_from(graph.nodes())\n",
    "\n",
    "    preprocessed_graph.node_codes = node_codes\n",
    "    preprocessed_graph.node_types = node_types\n",
    "        \n",
    "    # drop shortcut edges\n",
    "    exclude_codes = []\n",
    "    exclude_codes.append(edge_codes[list(edge_types).index(\"Subsumes\")])\n",
    "    exclude_codes.append(edge_codes[list(edge_types).index(\"Is a\")])\n",
    "\n",
    "    for (u, v, w), c in zip(graph.edges.data(\"edge_weight\"), edge_codes):\n",
    "        assert w is not None\n",
    "\n",
    "        # drop shortcut edges\n",
    "        if c in exclude_codes and w < 1.0:\n",
    "            continue\n",
    "\n",
    "        preprocessed_graph.add_edge(u, v, edge_weight=w, edge_code=c)\n",
    "\n",
    "    preprocessed_graph.edge_types = edge_types\n",
    "    \n",
    "    \n",
    "\n",
    "    return preprocessed_graph\n",
    "\n",
    "#2 load graph data \n",
    "#loading the full graph\n",
    "base_path = pathlib.Path(\n",
    "    \"/data/analysis/ag-reils/ag-reils-shared/cardioRS/data/2_datasets_pre/211110_anewbeginning\")\n",
    "G = nx.readwrite.gpickle.read_gpickle('/data/analysis/ag-reils/ag-reils-shared/cardioRS/data/2_datasets_pre/211110_anewbeginning/graph_full_211122.p')\n",
    "#2 preprocess now \n",
    "PG = preprocess_graph_heterogeneous(G)\n",
    "#2 create Triple Array\n",
    "tripleList=[]\n",
    "nodes=[]\n",
    "for u,v,data in PG.edges.data():\n",
    "    l=[]\n",
    "    l.append(u)\n",
    "    nodes.append(u)\n",
    "    l.append(data['edge_code'])\n",
    "    l.append(v)\n",
    "    nodes.append(v)\n",
    "    tripleList.append(l)\n",
    "\n",
    "#needs triples as ndarray - shape (n,3), dtype:str \n",
    "tripleArray=np.array(tripleList, dtype=str)\n",
    "print(len(tripleArray))\n",
    "del G\n",
    "del PG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169ea703-e7de-4db7-9dd2-d7c183ab77d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save nodelist and tripleArray as txt file \n",
    "'''with open(\"node_list.txt\", 'w') as f:   \n",
    "    for n in nodes:\n",
    "        f.write(str(n) +'\\n')\n",
    "\n",
    "with open('triple_array.txt', 'w') as f:\n",
    "    for liste in tripleArray:\n",
    "        row =liste[0] + ' ' + liste[1] + ' ' + liste[2]\n",
    "        f.write(str(row) + '\\n')\n",
    "\n",
    "# load node_list.txt \n",
    "nodes2 = []\n",
    "with open('node_list.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        nodes.append(str(line.strip()))\n",
    "\n",
    "tripleArray2=[]\n",
    "with open('triple_array.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        tripleArray2.append(l)\n",
    "        \n",
    "'''\n",
    "# save nodelist and triples array as pd.feather\n",
    "\n",
    "df_nodes = pd.DataFrame(nodes, columns = ['nodes'])\n",
    "df_triple = pd.DataFrame(tripleArray, columns = ['n1', 'relation', 'n2'])\n",
    "print(df_nodes.head())\n",
    "df_triple.head()\n",
    "\n",
    "df_nodes.to_feather('node_list.feather')\n",
    "df_triple.to_feather('triple_list.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "904eb09e-82cf-49b2-b934-a45fe8322a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 111111111111111111111111111111111111111111111111\n",
    "\n",
    "#! 1. pay attention to the embedding_dim variable and addapt it if necessary\n",
    "\n",
    "# function to load the embedding \n",
    "\n",
    "def load_embedding(checkpoint_path, model, emb_dim, tripleArray,inverse, cuda=False):\n",
    "    if cuda is False:\n",
    "        checkpoint = torch.load(PYKEEN_CHECKPOINTS.joinpath(checkpoint_path), map_location={'cuda:0':'cpu'}) \n",
    "        # checkpoint['entity_to_id_dict']) \n",
    "    else:\n",
    "        checkpoint = torch.load(PYKEEN_CHECKPOINTS.joinpath(checkpoint_path))\n",
    "    \n",
    "    \n",
    "    # checkpoint contains model and entity_to_id, realtion_to_id mapping that was used\n",
    "    # load these into Pykeen:\n",
    "    train_tf = TriplesFactory.from_labeled_triples(\n",
    "        triples = tripleArray, \n",
    "        create_inverse_triples=inverse, # default was True due 18.03.22\n",
    "        entity_to_id=checkpoint['entity_to_id_dict'],\n",
    "        relation_to_id=checkpoint['relation_to_id_dict'],\n",
    "    )\n",
    "    \n",
    "    # load the model and pass the train triple factory to the model\n",
    "    used_model = model(\n",
    "        triples_factory=train_tf,\n",
    "        embedding_dim=emb_dim, # 128 if RotatE or ComplEx (b.c. of complex integers)       # check necessary\n",
    "        random_seed=420\n",
    "    ) # 128 for complex embeddings\n",
    "    \n",
    "    \n",
    "    # get the embeddings\n",
    "    # class RepresentationModule\n",
    "    entity_RepModel = used_model.entity_representations[0] # check for more representations\n",
    "    try:\n",
    "        print(used_model.entity_representations[1])\n",
    "    except IndexError:\n",
    "        print('Index Error, no more entity_reps ')\n",
    "    \n",
    "    #TODO assert for entity_representations[1]\n",
    "    \n",
    "    # accses over checkpoint: \n",
    "    checkp_entities_val = list(checkpoint['entity_to_id_dict'].values())\n",
    "    embeddings = entity_RepModel(torch.tensor(checkp_entities_val, dtype=torch.long)) # error with NopePiece at dtype=torch.int()# cast list elements into tensors\n",
    "    #??embedding_dict = dict(zip(checkpoint['entity_to_id_dict'].keys(), embeddings.detach().numpy() )) #  Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.\n",
    "    df = pd.DataFrame(dict(nodes= checkpoint['entity_to_id_dict'].keys(), embeddings= list(embeddings.detach().numpy()) ))\n",
    "    del model\n",
    "    return df\n",
    "\n",
    "\n",
    "# takes necessary arguments and passes it to function. checks all necessary variables\n",
    "def helper(checkpoint_name, Model,emb_dim, emb_dict_name, inverse):\n",
    "    print(Model)\n",
    "    check = input(\"imported the right Model? inverse Triple used? /yes\")\n",
    "    if check != \"yes\": return 1\n",
    "    #load the triplesArray\n",
    "    triples = pd.read_feather('triple_list.feather').to_numpy()\n",
    "    checkp_path = checkpoint_name + \".pt\" #  \"'/home/tilingl/.data/pykeen/checkpoints/\"\n",
    "    df = load_embedding(checkp_path, Model, emb_dim, triples,inverse)\n",
    "    print(df)\n",
    "    emb_dic_path =\"embeddings/Embedding_dict_\" + emb_dict_name + \".feather\"\n",
    "    df.to_feather(emb_dic_path)\n",
    "    \n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa8b4e88-f947-43ee-b0cb-f0eaa1b39683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pykeen.models.unimodal.rotate.RotatE'>\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "imported the right Model? inverse Triple used? /yes yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No cuda devices were available. The model runs on CPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index Error, no more entity_reps \n",
      "                  nodes                                         embeddings\n",
      "0          OMOP_1000560  [0.26667762, -0.28079098, 0.22935323, -0.37219...\n",
      "1          OMOP_1000577  [0.24906819, -0.3066625, -0.186661, -0.2085861...\n",
      "2          OMOP_1000579  [-0.07856253, 0.14064436, 0.34394872, 0.140200...\n",
      "3          OMOP_1000599  [0.27964348, -0.1326068, 0.3160723, 0.1552092,...\n",
      "4          OMOP_1000600  [0.17873678, 0.21795434, -0.121780336, -0.3138...\n",
      "...                 ...                                                ...\n",
      "511286  OT_R-HSA-983695  [0.16453658, 0.17231901, 0.331063, 0.036961824...\n",
      "511287  OT_R-HSA-983705  [0.24189198, 0.060960844, 0.060015544, -0.3292...\n",
      "511288  OT_R-HSA-983712  [-0.120542586, 0.28354615, -0.2839713, 0.03951...\n",
      "511289  OT_R-HSA-991365  [0.34917247, -0.17682476, 0.11770615, -0.02588...\n",
      "511290  OT_R-HSA-997272  [-0.29147065, -0.071174115, 0.1486845, 0.14663...\n",
      "\n",
      "[511291 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 22222222222222222222222222222222222222222222\n",
    "\n",
    "# save embeddings/ embedding_dict as feather\n",
    "\n",
    "#! 1. modify the path according to the right Embedding checkpoint\n",
    "#! 2. Import the corresponding model from pykeen.models\n",
    "#! 3. Change the model variable to the relating model\n",
    "#! 4. adapt the name of the new Embedding_dict according to the checkpoint/model and Version of the embedding\n",
    "#! 5. check the embedding_dim variable in the load_embedding function\n",
    "\n",
    "\n",
    "from pykeen.models import RotatE # ! 128 if RotatE or ComplEx (b.c. of complex integers)\n",
    "#helper(checkpoint_name, Model,emb_dim, emb_dict_name, inverse_triple?)\n",
    "helper(\"RotatE_2_256_t2\",RotatE ,128, \"RotatE_2_256_t2_v2\",False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "193a45d3-47f4-4f5d-9271-6e35185ed792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No cuda devices were available. The model runs on CPU\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MuRE:\n\tsize mismatch for relation_representations.0._embeddings.weight: copying a param with shape torch.Size([326, 256]) from checkpoint, the shape in current model is torch.Size([163, 256]).\n\tsize mismatch for relation_representations.1._embeddings.weight: copying a param with shape torch.Size([326, 256]) from checkpoint, the shape in current model is torch.Size([163, 256]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_40345/4249912443.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     random_seed=600)\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mmy_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/DeepWalk/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   1483\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   1484\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MuRE:\n\tsize mismatch for relation_representations.0._embeddings.weight: copying a param with shape torch.Size([326, 256]) from checkpoint, the shape in current model is torch.Size([163, 256]).\n\tsize mismatch for relation_representations.1._embeddings.weight: copying a param with shape torch.Size([326, 256]) from checkpoint, the shape in current model is torch.Size([163, 256])."
     ]
    }
   ],
   "source": [
    "# check out MuRE second entity_representation and wether it is important or not \n",
    "#MuRe_model = helper(\"MuRE_slcwa_t1\",MuRE ,256, \"MuRE_slcwa_t1_v2\")\n",
    "checkpoint = torch.load(PYKEEN_CHECKPOINTS.joinpath(\"MuRE_slcwa_t1.pt\"), map_location={'cuda:0':'cpu'}) \n",
    "#triples = pd.read_feather('triple_list.feather').to_numpy()\n",
    "#train = TriplesFactory.from_labeled_triples(\n",
    "#    triples=triples,\n",
    "#    entity_to_id=checkpoint['entity_to_id_dict'],\n",
    "#    relation_to_id=checkpoint['relation_to_id_dict'],\n",
    "\n",
    "#)\n",
    "checkpoint[\n",
    "#now load the model and pass the train triples factory to the model\n",
    "my_model = MuRE(\n",
    "    triples_factory=train, \n",
    "    embedding_dim=256,\n",
    "    random_seed=600)\n",
    "\n",
    "my_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "print(my_model.entity_representations[0], my_model.entity_representations[1])\n",
    "\n",
    "\n",
    "\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n",
    "# in comparison to old method: check instant avaiability of variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a885890c-d709-47cf-90e1-505ed37f8c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['epoch', 'loss', 'model_state_dict', 'optimizer_state_dict', 'lr_scheduler_state_dict', 'checksum', 'random_seed', 'stopper_dict', 'random_state', 'np_random_state', 'torch_random_state', 'torch_cuda_random_state', 'best_epoch_model_checkpoint', 'relation_to_id_dict', 'entity_to_id_dict']) 5\n",
      "\n",
      " odict_keys(['entity_representations.0._embeddings.weight', 'entity_representations.1._embeddings.weight', 'entity_representations.2._embeddings.weight', 'relation_representations.0._embeddings.weight', 'relation_representations.1._embeddings.weight'])\n"
     ]
    }
   ],
   "source": [
    "print(checkpoint.keys(), len(checkpoint['model_state_dict']))\n",
    "file = checkpoint['model_state_dict']\n",
    "print('\\n',file.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4bd315-2613-4ef8-a6ab-f0f48e110853",
   "metadata": {},
   "outputs": [],
   "source": [
    "triples = pd.read_feather('triple_list.feather').to_numpy()\n",
    "\n",
    "#returns dataframe with dict of nodes:embedding\n",
    "from pykeen.models import ComplEx # !\n",
    "df = load_embedding('/home/tilingl/.data/pykeen/checkpoints/ComplEx_t1.pt', ComplEx, triples)\n",
    "print(df)\n",
    "df.to_feather('embeddings/Embedding_dict_ComplEx_2_t1_v2.feather')\n",
    "\n",
    "# notitz: MuRE hat noch eine entity representation\n",
    "\n",
    "# Explaining variable baselines\n",
    "# model.pt -> baseline\n",
    "# model_2.pt -> fixed some errors in baseline\n",
    "# model_t1.pt -> first hyperparameter tuning \n",
    "# model_256/128.pt -> embedding_dim specification for the used model\n",
    "# model_v2.pt -> due to a missmatch of embedding model and checkpoint model this is version 2 with matched models. All embeddings after february where missmatched with RotatE\n",
    "\n",
    "# beginning at 27.03.22 new script and new approach to achieve embeddins!\n",
    "# emb_runNAME called rus in wandb\n",
    "# emb_runNAME_tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92024847-3523-421a-9b76-94dcf0d116dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
