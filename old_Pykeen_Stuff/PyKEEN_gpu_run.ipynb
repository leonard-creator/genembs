{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da5ef88a-868e-4ac9-9d5c-af5140d34d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Run of whole Graph on the GPU \n",
    "\n",
    "import pykeen\n",
    "from pykeen.pipeline import pipeline\n",
    "import networkx as nx\n",
    "import pathlib\n",
    "from random import sample\n",
    "import pandas as pd\n",
    "from pykeen.triples import TriplesFactory\n",
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b69691e-98de-45d6-95b5-253e9855beda",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "#              NOTES             #\n",
    "##################################\n",
    "\n",
    "# sample function is causing the problem with splitting function (ratio)\n",
    "# same error in default pipeline, but at the beginning not at the end of calculations\n",
    "\n",
    "# use EGO Graph for finding small subgraph\n",
    "# prepare checkpoint notebook with embedding accses for Ben\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8700666f-5e19-44e9-abe2-50b07bcb2688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/nebw/ehrgraphs/blob/master/ehrgraphs/data/data.py#L82-L117\n",
    "def preprocess_graph_heterogeneous(graph: nx.Graph):\n",
    "    edge_types = []\n",
    "    for u, v, data in graph.edges.data():\n",
    "        edge_types.append(data[\"edge_type\"])\n",
    "\n",
    "    #edge_codes, edge_types = pd.factorize(edge_types)\n",
    "    \n",
    "    node_types = []\n",
    "    for n, data in graph.nodes.data():\n",
    "        node_types.append(data[\"node_type\"])\n",
    "\n",
    "    node_codes, node_types = pd.factorize(node_types)\n",
    "\n",
    "    preprocessed_graph = nx.DiGraph()\n",
    "    preprocessed_graph.add_nodes_from(graph.nodes())\n",
    "\n",
    "    preprocessed_graph.node_codes = node_codes\n",
    "    preprocessed_graph.node_types = node_types\n",
    "        \n",
    "    # drop shortcut edges\n",
    "    '''exclude_codes = []\n",
    "    exclude_codes.append(edge_codes[list(edge_types).index(\"Subsumes\")])\n",
    "    exclude_codes.append(edge_codes[list(edge_types).index(\"Is a\")])\n",
    "    \n",
    "\n",
    "    for (u, v, w), c in zip(graph.edges.data(\"edge_weight\"), edge_codes):\n",
    "        assert w is not None\n",
    "\n",
    "        # drop shortcut edges\n",
    "        if c in exclude_codes and w < 1.0:\n",
    "            continue\n",
    "        \n",
    "        preprocessed_graph.add_edge(u, v, edge_weight=w, edge_code=c)\n",
    "        '''\n",
    "    # ---- #\n",
    "    edge_codes, num_counts = np.unique(edge_types, return_counts=True)\n",
    "    fraction = num_counts / len(edge_types)\n",
    "    valid_edge_codes = edge_codes[fraction >= 0.01]\n",
    "    print(len(valid_edge_codes), valid_edge_codes)\n",
    "    \n",
    "    for (u,v,w), t in zip(graph.edges.data(\"edge_weight\"), edge_types):\n",
    "        assert w is not None\n",
    "        \n",
    "        # only most relevant relations\n",
    "        if t in valid_edge_codes and w >=1.0:\n",
    "            preprocessed_graph.add_edge(u, v, edge_weight=w, edge_code=t)\n",
    "        \n",
    "        else : continue\n",
    "    # ---- #    \n",
    "    \n",
    "    preprocessed_graph.edge_types = edge_types\n",
    "\n",
    "    return preprocessed_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acdbe68a-cd81-4f5b-950a-9063a4a56dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the full graph\n",
    "base_path = pathlib.Path(\n",
    "    \"/data/analysis/ag-reils/ag-reils-shared/cardioRS/data/2_datasets_pre/211110_anewbeginning\")\n",
    "G = nx.readwrite.gpickle.read_gpickle('/data/analysis/ag-reils/ag-reils-shared/cardioRS/data/2_datasets_pre/211110_anewbeginning/graph_full_211122.p')\n",
    "\n",
    "# building preprocessed ego graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36873bdb-db58-41e7-9bcd-b68b7ab330ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 ['Interacts with' 'Is a' 'Is associated with' 'Subsumes']\n",
      "2383019\n"
     ]
    }
   ],
   "source": [
    "# -----TEST -----#\n",
    "SG = preprocess_graph_heterogeneous(G)\n",
    "print(len(list(SG.edges())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "13045e27-f6ef-4e9e-b2a5-b53e5b21cc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45664725\n"
     ]
    }
   ],
   "source": [
    "print(len(list(G.edges())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7f0019e3-ccd1-4cce-8550-ddd2364ad840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length bevor sampling:  2383019\n"
     ]
    }
   ],
   "source": [
    "# ---------- collect the most frequent relations for subgraph splitting ---- #\n",
    "\n",
    "#SG = preprocess_graph_heterogeneous(G)\n",
    "\n",
    "\n",
    "tripleList=[]\n",
    "nodes=[]\n",
    "for u,v,data in SG.edges.data():\n",
    "    l=[]\n",
    "    l.append(u)\n",
    "    nodes.append(u)\n",
    "    l.append(data['edge_code'])\n",
    "    l.append(v)\n",
    "    nodes.append(v)\n",
    "    tripleList.append(l)\n",
    "print('length bevor sampling: ', len(tripleList))\n",
    "#needs triples as ndarray - shape (n,3), dtype:str \n",
    "tripleArray=np.array(sample(tripleList,1000), dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bea92736-5593-4ad7-bd43-b651c654f3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2383019\n"
     ]
    }
   ],
   "source": [
    "# -------- directly create TRIPLES ------- #\n",
    "\n",
    "#SG = preprocess_graph_heterogeneous(G)\n",
    "\n",
    "tripleList=[]\n",
    "nodes=[]\n",
    "for u,v,data in SG.edges.data():\n",
    "    l=[]\n",
    "    l.append(u)\n",
    "    nodes.append(u)\n",
    "    l.append(data['edge_code'])\n",
    "    l.append(v)\n",
    "    nodes.append(v)\n",
    "    tripleList.append(l)\n",
    "\n",
    "#needs triples as ndarray - shape (n,3), dtype:str \n",
    "tripleArray=np.array(tripleList, dtype=str)\n",
    "print(len(tripleArray))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6184e30-b050-436a-93fe-bf6543f9b166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete NETWROKX Graph\n",
    "SG.clear()\n",
    "del SG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67828aa3-e5e1-4123-9c82-9ec07c60d738",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using automatically assigned random_state=3971841365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No cuda devices were available. The model runs on CPU\n",
      "No random seed is specified. This may lead to non-reproducible results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used LOSS :  MarginRankingLoss(\n",
      "  (margin_activation): ReLU()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "885d071b92fe434d88860d84cf1a0ac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epochs on cpu:   0%|          | 0/100 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cpu:   0%|          | 0/16756 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cpu:   0%|          | 0/16756 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0903be7388fb4e67bb2098b2c9f6e05e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cpu:   0%|          | 0/16756 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ----------- directly loading Triples into PyKEEN ------ #\n",
    "\n",
    "tf2 = TriplesFactory.from_labeled_triples(tripleArray, create_inverse_triples=True)\n",
    "\n",
    "print(tf2.get_most_frequent_relations(3))\n",
    "# ----------- Splitting into training, testing, validation ---#\n",
    "\n",
    "training_factory, testing_factory, validation_factory = tf2.split([0.9 ,0.01])\n",
    "\n",
    "\n",
    "# ----------- Training without evaluation ------------ #\n",
    "\n",
    "# Pick a model\n",
    "from pykeen.models import TransE\n",
    "\n",
    "kwargs={'triples_factory': training_factory , 'loss': None, 'predict_with_sigmoid':False, 'preferred_device':None, 'random_seed':None}\n",
    "\n",
    "tf2_model = TransE(**kwargs, embedding_dim=64) # >64, 256\n",
    "\n",
    "# Pick an optimizer from Torch\n",
    "from torch.optim import Adam\n",
    "\n",
    "optimizer = Adam(params=tf2_model.get_grad_params())\n",
    "\n",
    "# stopper \n",
    "from pykeen.stoppers import EarlyStopper\n",
    "from pykeen.evaluation import RankBasedEvaluator\n",
    "evaluator=RankBasedEvaluator()\n",
    "\n",
    "Stopper = EarlyStopper(\n",
    "        model=tf2_model,\n",
    "        evaluator=evaluator,\n",
    "        training_triples_factory=training_factory,\n",
    "        evaluation_triples_factory=validation_factory,\n",
    "        #result_tracker=_result_tracker,\n",
    "        #**stopper_kwargs,\n",
    "    )\n",
    "\n",
    "# Pick a training approach (sLCWA or LCWA)\n",
    "from pykeen.training import SLCWATrainingLoop\n",
    "\n",
    "training_loop = SLCWATrainingLoop(\n",
    "\n",
    "    model=tf2_model,\n",
    "\n",
    "    triples_factory=training_factory,\n",
    "\n",
    "    optimizer=optimizer,\n",
    "\n",
    ")\n",
    "print('used LOSS : ', training_loop.loss)\n",
    "\n",
    "\n",
    "# Train like Cristiano Ronaldo\n",
    "\n",
    "_ = training_loop.train(\n",
    "\n",
    "    triples_factory=training_factory,\n",
    "\n",
    "    num_epochs=100, # ! ! !\n",
    "\n",
    "    batch_size=256,\n",
    "    \n",
    "    stopper= Stopper, \n",
    "    \n",
    "    num_workers=4\n",
    "    # result_tracker='wandb'\n",
    "    # result_tracker_kwargs=\n",
    ")\n",
    "\n",
    "print(Stopper.best_epoch, Stopper.best_metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcf2ffa-9afe-4a39-9b8b-4da52402857a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RepresentationModule\n",
    "entity_RepModel = tf2_model.entity_representations[0] # check for more representations\n",
    "\n",
    "# collect all nodes out of SG2 that contain an edge\n",
    "nodes = [u for u,v,data in SG2.edges.data()]\n",
    "for u,v,data in SG2.edges.data():\n",
    "    nodes.append(v)\n",
    "print(len(nodes))\n",
    "#filter out duplicate nodes\n",
    "nodes = list(dict.fromkeys(nodes))\n",
    "\n",
    "\n",
    "# make all entities to ids\n",
    "embedding_dict={}\n",
    "all_entities = tf2.entities_to_ids(nodes)\n",
    "\n",
    "embeddings = entity_RepModel(torch.tensor(all_entities, dtype=torch.int)) # cast list elements into tensors\n",
    "\n",
    "embedding_dict = dict(zip(nodes, embeddings2.detach().numpy() )) #  Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "db7e214f-0853-46eb-9369-7044d3398aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pykeen.triples.triples_factory:applying cutoff of 3 to TriplesFactory(num_entities=486362, num_relations=6, num_triples=2383019, inverse_triples=True)\n",
      "WARNING:pykeen.utils:using automatically assigned random_state=2051614596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pykeen.triples.splitting:done splitting triples to groups of sizes [1658357, 23830, 214472]\n",
      "WARNING:pykeen.pipeline.api:No random seed is specified. Setting to 1168089376.\n",
      "WARNING:pykeen.utils:No cuda devices were available. The model runs on CPU\n",
      "INFO:pykeen.triples.triples_factory:Creating inverse triples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8947073cd474d57ba977bee7d18be4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epochs on cpu:   0%|          | 0/100 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4e8e49b477341278022e4cb6d69e592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cpu:   0%|          | 0/16756 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25920/1593221487.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m results = pipeline(\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_factory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtesting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtesting_factory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/DeepWalk/lib/python3.8/site-packages/pykeen/pipeline/api.py\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(dataset, dataset_kwargs, training, testing, validation, evaluation_entity_whitelist, evaluation_relation_whitelist, model, model_kwargs, interaction, interaction_kwargs, dimensions, loss, loss_kwargs, regularizer, regularizer_kwargs, optimizer, optimizer_kwargs, clear_optimizer, lr_scheduler, lr_scheduler_kwargs, training_loop, training_loop_kwargs, negative_sampler, negative_sampler_kwargs, epochs, training_kwargs, stopper, stopper_kwargs, evaluator, evaluator_kwargs, evaluation_kwargs, result_tracker, result_tracker_kwargs, metadata, device, random_seed, use_testing_data, evaluation_fallback, filter_validation_when_testing, use_tqdm)\u001b[0m\n\u001b[1;32m   1066\u001b[0m     \u001b[0;31m# Train like Cristiano Ronaldo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m     \u001b[0mtraining_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m     losses = training_loop_instance.train(\n\u001b[0m\u001b[1;32m   1069\u001b[0m         \u001b[0mtriples_factory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0mstopper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstopper_instance\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/DeepWalk/lib/python3.8/site-packages/pykeen/training/training_loop.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, triples_factory, num_epochs, batch_size, slice_size, label_smoothing, sampler, continue_training, only_size_probing, use_tqdm, use_tqdm_batch, tqdm_kwargs, stopper, result_tracker, sub_batch_size, num_workers, clear_optimizer, checkpoint_directory, checkpoint_name, checkpoint_frequency, checkpoint_on_failure, drop_last, callbacks)\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses_per_epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m             result = self._train(\n\u001b[0m\u001b[1;32m    320\u001b[0m                 \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/DeepWalk/lib/python3.8/site-packages/pykeen/training/training_loop.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, triples_factory, training_instances, num_epochs, batch_size, slice_size, label_smoothing, sampler, continue_training, only_size_probing, use_tqdm, use_tqdm_batch, tqdm_kwargs, stopper, result_tracker, sub_batch_size, num_workers, save_checkpoints, checkpoint_path, checkpoint_frequency, checkpoint_on_failure_file_path, best_epoch_model_file_path, last_best_epoch, drop_last, callbacks)\u001b[0m\n\u001b[1;32m    638\u001b[0m                     \u001b[0;31m# After changing applying the gradients to the embeddings, the model is notified that the forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m                     \u001b[0;31m# constraints are no longer applied\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_parameter_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m                     \u001b[0;31m# For testing purposes we're only interested in processing one batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/DeepWalk/lib/python3.8/site-packages/pykeen/models/base.py\u001b[0m in \u001b[0;36mpost_parameter_update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    754\u001b[0m         \u001b[0;31m# make sure to call this first, to reset regularizer state!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_parameter_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentity_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_parameter_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelation_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_parameter_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/DeepWalk/lib/python3.8/site-packages/pykeen/nn/emb.py\u001b[0m in \u001b[0;36mpost_parameter_update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;31m# apply constraints in-place\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstrainer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m     def forward(\n",
      "\u001b[0;32m~/miniconda3/envs/DeepWalk/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(input, p, dim, eps, out)\u001b[0m\n\u001b[1;32m   4445\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4446\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4447\u001b[0;31m         \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_min\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4448\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4449\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/DeepWalk/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(self, p, dim, keepdim, dtype)\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_unary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpivot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_infos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/DeepWalk/lib/python3.8/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[1;32m   1463\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1465\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdim\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1466\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#####################\n",
    "# Easy Pipeline Way #\n",
    "####################\n",
    "\n",
    "tf2 = TriplesFactory.from_labeled_triples(tripleArray, create_inverse_triples=True)\n",
    "\n",
    "print(tf2.get_most_frequent_relations(3))\n",
    "# ----------- Splitting into training, testing, validation ---#\n",
    "\n",
    "training_factory, testing_factory, validation_factory = tf2.split([0.9 ,0.01])\n",
    "\n",
    "\n",
    "# --\n",
    "\n",
    "\n",
    "results = pipeline(\n",
    "    training = training_factory,\n",
    "    testing = testing_factory,\n",
    "    validation = validation_factory,\n",
    "    loss='marginranking',\n",
    "    loss_kwargs=dict(margin=1),\n",
    "    model = tf2_model,\n",
    "    epochs = 100, \n",
    "    training_loop = 'sLCWA',\n",
    "    negative_sampler = 'basic',\n",
    "    evaluator = 'rankbased',\n",
    "    stopper = 'early',\n",
    "    stopper_kwargs=dict(frequency=5, patience=2, relative_delta=0.002)\n",
    ")\n",
    "\n",
    "results.plot_losses()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde7c0d9-072f-4909-898e-c42d06b65e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE INTO DATAFRAME \n",
    "dsaf\n",
    "items = embedding_dictitems.items()\n",
    "\n",
    "data_list = list()\n",
    "\n",
    "df = pd.DataFrame(data_list)\n",
    "df.to_csv(index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
